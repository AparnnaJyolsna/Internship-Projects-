{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10232940,"sourceType":"datasetVersion","datasetId":6327195}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntorch.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def walk_through_dir(dir_path):\n    for dirpath, dirnames, filenames in os.walk(dir_path):\n        print(f\"There are {len(dirnames)} directories and {len(filenames)} files in '{dirpath}'.\")\n        for filename in filenames:\n            print(f\"Found file: {filename}\")\n            \n\ninput_dir = '/kaggle/input/readings/'\n\n\nwalk_through_dir(input_dir)\n\n\ncsv_file1 = '/kaggle/input/readings/u_bar_8.csv'\ndata1 = pd.read_csv(csv_file1)\n\n\nprint(f\"Contents of the file: {os.path.basename(csv_file1)}\")\nprint(data1.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\n\ncsv_file1 = '/kaggle/input/readings/u_bar_8.csv'\n\ndata1 = pd.read_csv(csv_file1)\n\n\ntime1 = data1.iloc[:, 0]  \npressure1 = data1.iloc[:, 1]\n\n\ntotal_readings = len(time1)\nwindow_size = 500  \n\n\nnum_graphs = (total_readings + window_size - 1) // window_size  \n\n\nfigures_list1 = []\n\n\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = (i + 1) * window_size\n\n    \n    if end_idx > total_readings:\n        end_idx = total_readings\n\n    \n    time_segment1 = time1[start_idx:end_idx]\n    pressure_segment1 = pressure1[start_idx:end_idx]\n\n   \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(time_segment1, pressure_segment1, label=f\"Pressure vs Time (Window {i+1})\", color='b')\n    ax.set_title(f\"Pressure vs Time from {os.path.basename(csv_file1)} - Window {i+1}\")\n    ax.set_xlabel('Time1 (seconds)')\n    ax.set_ylabel('Pressure1 (Pascal)')\n    ax.grid(True)\n    ax.legend()\n\n   \n    figures_list1.append(fig)\n\n   \n    plt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ncsv_file1 = '/kaggle/input/readings/u_bar_8.csv'\n\n# Load the data from the CSV file into a pandas DataFrame\ndata1 = pd.read_csv(csv_file1, header=None, names=[\"Time1\", \"Pressure1\"])\n\n\n# Extract time and pressure columns\ntime1 = data1.iloc[:, 0]  \npressure1 = data1.iloc[:, 1]\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(time1, pressure1, label=\"Pressure vs Time\", color='b')\nplt.title(f\"Pressure vs Time from {os.path.basename(csv_file1)}\")\nplt.xlabel('Time (seconds)')\nplt.ylabel('Pressure (Pascal)')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the CSV file into a pandas dataframe\ncsv_file2 = '/kaggle/input/readings/u_bar_9.csv'\ndata2 = pd.read_csv(csv_file2)\n\n# Check the first few rows of the dataframe to understand its structure\nprint(f\"Contents of the file: {os.path.basename(csv_file2)}\")\nprint(data2.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file2 = '/kaggle/input/readings/u_bar_9.csv'\n\n# Load the data from the CSV file into a pandas DataFrame\ndata2 = pd.read_csv(csv_file2, header=None, names=[\"Time2\", \"Pressure2\"])\n\n\n# Extract time and pressure columns\ntime2 = data2.iloc[:, 0]  \npressure2 = data2.iloc[:, 1]\n\nplt.figure(figsize=(10, 6))\nplt.plot(time2, pressure2, label=\"Pressure vs Time\", color='b')\nplt.title(f\"Pressure vs Time from {os.path.basename(csv_file2)}\")\nplt.xlabel('Time (seconds)')\nplt.ylabel('Pressure (Pascal)')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\n\n\ncsv_file2 = '/kaggle/input/readings/u_bar_9.csv'\n\n\ndata2 = pd.read_csv(csv_file2)\n\n\ntime2 = data2.iloc[:, 0]  \npressure2 = data2.iloc[:, 1]\n\ntotal_readings = len(time2)\nwindow_size = 500  \n\n\nnum_graphs = (total_readings + window_size - 1) // window_size  \n\nfigures_list2 = []\n\n\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = (i + 1) * window_size\n\n    \n    if end_idx > total_readings:\n        end_idx = total_readings\n\n    \n    time_segment2 = time2[start_idx:end_idx]\n    pressure_segment2 = pressure2[start_idx:end_idx]\n\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(time_segment2, pressure_segment2, label=f\"Pressure vs Time (Window {i+1})\", color='b')\n    ax.set_title(f\"Pressure vs Time from {os.path.basename(csv_file2)} - Window {i+1}\")\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Pressure (Pascal)')\n    ax.grid(True)\n    ax.legend()\n\n    \n    figures_list2.append(fig)\n\n   \n    plt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import ShortTimeFFT\nfrom scipy.signal.windows import gaussian\n\ndef window_to_spectrogram(time_values, amplitude_values, T_x, g_std=8, win_length=50, hop_size=10, mfft_length=50, scale_to='magnitude'):\n\n    x = amplitude_values\n\n\n    T_x, N = 1 / 50, 1000\n    g_std = 8\n    w = gaussian(50, std=g_std, sym=True)\n\n\n    SFT = ShortTimeFFT(w, hop=10, fs=1/T_x, mfft=50, scale_to='magnitude')  \n    Sx = SFT.stft(x)  \n\n    assert len(time_values) == len(x), \"Time values length must match the signal length.\"\n\n\n    plt.figure(figsize=(8, 5))  \n    plt.plot(time_values, x, label='Signal', color='b') \n    plt.title(\"Original Signal\")  \n    plt.xlabel(\"Time [s]\")  \n    plt.ylabel(\"Amplitude\")  \n    plt.grid(True) \n    plt.legend()  \n    plt.tight_layout()  \n    plt.show()  \n\n    # Plotting the result (STFT)\n    fig1, ax1 = plt.subplots(figsize=(6., 4.))  # Create a figure for the plot\n    t_lo, t_hi = SFT.extent(N)[:2]  # Time range of the plot\n    \n    # Set the title and axis labels\n    ax1.set_title(rf\"STFT ({SFT.m_num*SFT.T:g}$\\,s$ Gaussian window, \" +\n                  rf\"$\\sigma_t={g_std*SFT.T}\\,$s)\")\n    ax1.set(xlabel=f\"Time $t$ in seconds ({SFT.p_num(N)} slices, \" +\n                   rf\"$\\Delta t = {SFT.delta_t:g}\\,$s)\",\n            ylabel=f\"Freq. $f$ in Hz ({SFT.f_pts} bins, \" +\n                   rf\"$\\Delta f = {SFT.delta_f:g}\\,$Hz)\",\n            xlim=(time_values[0], time_values[-1]))  \n\n    # Plot the magnitude of the STFT as an image, using time_values for the x-axis\n    im1 = ax1.imshow(abs(Sx), origin='lower', aspect='auto',\n                     extent=[time_values[0], time_values[-1], 0, SFT.f_pts], cmap='viridis')\n    \n    # Add a colorbar\n    fig1.colorbar(im1, label=\"Magnitude $|S_x(t, f)|$\")\n    \n    # Shade areas where window slices stick out to the side\n    for t0_, t1_ in [(t_lo, SFT.lower_border_end[0] * SFT.T),\n                     (SFT.upper_border_begin(N)[0] * SFT.T, t_hi)]:\n        ax1.axvspan(t0_, t1_, color='w', linewidth=0, alpha=.2)\n    \n    # Mark the signal borders with vertical lines\n    for t_ in [0, N * SFT.T]:\n        ax1.axvline(t_, color='y', linestyle='--', alpha=0.5)\n\n    # Display legend\n    ax1.legend()\n    \n    # Adjust layout and show the plot\n    fig1.tight_layout()\n    plt.show() \n\n#time_values = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0, 4.2, 4.4, 4.6, 4.8, 5.0])\n#amplitude_values = np.array([10, 15, 20, 18, 10, 5, 0, -5, -10, -12, -10, -5, 0, 5, 10, 12, 10, 8, 5, 3, 10, 13, 7, 8, 8, 9])\n#window_to_spectrogram(time_values, amplitude_values, T_x=1/50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal.windows import gaussian\nfrom scipy.signal import ShortTimeFFT\n\n\ncsv_file1 = '/kaggle/input/readings/u_bar_8.csv'\ndata1 = pd.read_csv(csv_file1)\n\n\ntime1 = data1.iloc[:, 0]  \npressure1 = data1.iloc[:, 1]\n\n\ntotal_readings = len(time1)\nwindow_size = 500  \nnum_graphs = (total_readings + window_size - 1) // window_size  \n\n\ndef window_to_spectrogram(time_values, amplitude_values, T_x, g_std=8, win_length=50, hop_size=10, mfft_length=50, scale_to='magnitude'):\n    x = amplitude_values\n    w = gaussian(win_length, std=g_std, sym=True)\n    SFT = ShortTimeFFT(w, hop=hop_size, fs=1/T_x, mfft=mfft_length, scale_to=scale_to)\n    Sx = SFT.stft(x)\n    \n    \n    plt.figure(figsize=(8, 5))\n    plt.plot(time_values, x, label='Signal', color='b')\n    plt.title(\"Original Signal\")\n    plt.xlabel(\"Time [s]\")\n    plt.ylabel(\"Amplitude\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    \n    fig1, ax1 = plt.subplots(figsize=(6., 4.))\n    t_lo, t_hi = SFT.extent(len(x))[:2]\n    ax1.set_title(rf\"STFT ({SFT.m_num*SFT.T:g}$\\,s$ Gaussian window, $\\sigma_t={g_std*SFT.T}\\,$s)\")\n    ax1.set(xlabel=\"Time $t$ in seconds\", ylabel=\"Freq. $f$ in Hz\")\n    im1 = ax1.imshow(abs(Sx), origin='lower', aspect='auto', extent=[time_values[0], time_values[-1], 0, SFT.f_pts], cmap='viridis')\n    fig1.colorbar(im1, label=\"Magnitude $|S_x(t, f)|$\")\n    \n    plt.tight_layout()\n    plt.show()\n\n    \n    plt.close(fig1)\n    return fig1\n\nspectrograms0 = []\n\n\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = min((i + 1) * window_size, total_readings)\n\n    time_segment1 = time1[start_idx:end_idx].to_numpy()\n    pressure_segment1 = pressure1[start_idx:end_idx].to_numpy()\n\n    \n    print(f\"Processing window {i+1}/{num_graphs} of spectrograms0\")\n    fig1_list = window_to_spectrogram(time_segment1, pressure_segment1, T_x=1/50)\n    spectrograms0.append(fig1_list)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal.windows import gaussian\nfrom scipy.signal import ShortTimeFFT\n\n\ncsv_file2 = '/kaggle/input/readings/u_bar_9.csv'\ndata2 = pd.read_csv(csv_file2)\n\n\ntime2 = data2.iloc[:, 0]  \npressure2 = data2.iloc[:, 1]\n\n\ntotal_readings = len(time1)\nwindow_size = 500  \nnum_graphs = (total_readings + window_size - 1) // window_size  \n\n\ndef window_to_spectrogram(time_values, amplitude_values, T_x, g_std=8, win_length=50, hop_size=10, mfft_length=50, scale_to='magnitude'):\n    x = amplitude_values\n    w = gaussian(win_length, std=g_std, sym=True)\n    SFT = ShortTimeFFT(w, hop=hop_size, fs=1/T_x, mfft=mfft_length, scale_to=scale_to)\n    Sx = SFT.stft(x)\n    \n    \n    plt.figure(figsize=(8, 5))\n    plt.plot(time_values, x, label='Signal', color='b')\n    plt.title(\"Original Signal\")\n    plt.xlabel(\"Time [s]\")\n    plt.ylabel(\"Amplitude\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    \n    fig2, ax2 = plt.subplots(figsize=(6., 4.))\n    t_lo, t_hi = SFT.extent(len(x))[:2]\n    ax2.set_title(rf\"STFT ({SFT.m_num*SFT.T:g}$\\,s$ Gaussian window, $\\sigma_t={g_std*SFT.T}\\,$s)\")\n    ax2.set(xlabel=\"Time $t$ in seconds\", ylabel=\"Freq. $f$ in Hz\")\n    im2 = ax2.imshow(abs(Sx), origin='lower', aspect='auto', extent=[time_values[0], time_values[-1], 0, SFT.f_pts], cmap='viridis')\n    fig2.colorbar(im2, label=\"Magnitude $|S_x(t, f)|$\")\n    \n    plt.tight_layout()\n    plt.show()\n\n    \n    plt.close(fig2)\n    return fig2\n\nspectrograms1 = []\n\n\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = min((i + 1) * window_size, total_readings)\n\n    time_segment2 = time2[start_idx:end_idx].to_numpy()\n    pressure_segment2 = pressure2[start_idx:end_idx].to_numpy()\n\n    \n    print(f\"Processing window {i+1}/{num_graphs} of spectrograms1\")\n    fig2_list = window_to_spectrogram(time_segment2, pressure_segment2, T_x=1/50)\n    spectrograms1.append(fig2_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal.windows import gaussian\nfrom scipy.signal import ShortTimeFFT\n\n# Load CSV data\ncsv_file1 = '/kaggle/input/readings/u_bar_8.csv'\ndata1 = pd.read_csv(csv_file1)\n\n# Extract time and pressure\ntime1 = data1.iloc[:, 0]  \npressure1 = data1.iloc[:, 1]\n\n# Configuration\ntotal_readings = len(time1)\nwindow_size = 500  # Readings per graph\nnum_graphs = (total_readings + window_size - 1) // window_size  # Number of graphs\n\n# Function to generate spectrograms\ndef window_to_spectrogram(time_values, amplitude_values, T_x, g_std=8, win_length=50, hop_size=10, mfft_length=50, scale_to='magnitude'):\n    x = amplitude_values\n    w = gaussian(win_length, std=g_std, sym=True)\n    SFT = ShortTimeFFT(w, hop=hop_size, fs=1/T_x, mfft=mfft_length, scale_to=scale_to)\n    Sx1 = SFT.stft(x)\n    \n    # Return the spectrogram as a numpy array for further processing\n    return abs(Sx1)  # Only return the magnitude of the spectrogram\n\nspectrograms1 = []\n\n# Process each segment and create spectrograms\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = min((i + 1) * window_size, total_readings)\n\n    time_segment1 = time1[start_idx:end_idx].to_numpy()\n    pressure_segment1 = pressure1[start_idx:end_idx].to_numpy()\n\n    # Generate spectrogram and store it\n    print(f\"Processing window SET I {i+1}/{num_graphs}\")\n    Sx1 = window_to_spectrogram(time_segment1, pressure_segment1, T_x=1/50)\n    spectrograms1.append(Sx1)\n\n# Function to divide a spectrogram into 10 equal segments\ndef divide_into_segments(spectrogram, num_segments=10):\n    # Determine the number of rows in the spectrogram\n    num_rows1 = spectrogram.shape[0]\n    \n    # Calculate the number of rows per segment (can be rounded)\n    rows_per_segment1 = num_rows1 // num_segments\n    segments1 = []\n    \n    for i in range(num_segments):\n        start_row1 = i * rows_per_segment1\n        # For the last segment, we take the remaining rows to handle any remainder\n        end_row1 = (i + 1) * rows_per_segment1 if i < num_segments - 1 else num_rows1\n        segment = spectrogram[start_row1:end_row1, :]\n        \n        # Pad the segment to ensure consistent shape (optional, depends on your needs)\n        if segment.shape[0] < rows_per_segment1:\n            pad_amount1 = rows_per_segment1 - segment.shape[0]\n            segment = np.pad(segment, ((0, pad_amount1), (0, 0)), mode='constant')\n        \n        segments1.append(segment)\n    \n    # Ensure all segments are the same shape (same number of rows and columns)\n    max_rows = max([seg.shape[0] for seg in segments1])\n    max_cols = max([seg.shape[1] for seg in segments1])\n    \n    # Pad all segments to the same size\n    padded_segments1 = [np.pad(seg, ((0, max_rows - seg.shape[0]), (0, max_cols - seg.shape[1])), mode='constant') for seg in segments1]\n    \n    return padded_segments1\n\n# Convert spectrograms into 10 x n x n tensors\nspectrogram_tensors_file0 = []\n\n# Process each spectrogram and divide into 10 segments\nfor i, Sx1 in enumerate(spectrograms1):\n    print(f\"Dividing spectrogram SET I {i+1}/{len(spectrograms1)} into 10 segments.\")\n    segments1 = divide_into_segments(Sx1, num_segments=10)\n    \n    # Convert the list of segments into a numpy array (tensor)\n    tensor = np.array(segments1)\n    \n    # Append the tensor to the list\n    spectrogram_tensors_file0.append(tensor)\n\n# Now spectrogram_tensors_file0 contains 200 tensors, each of shape (10, n, n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal.windows import gaussian\nfrom scipy.signal import ShortTimeFFT\n\n# Load CSV data\ncsv_file2 = '/kaggle/input/readings/u_bar_9.csv'\ndata2 = pd.read_csv(csv_file2)\n\n# Extract time and pressure\ntime2 = data2.iloc[:, 0]  \npressure2 = data2.iloc[:, 1]\n\n# Configuration\ntotal_readings = len(time2)\nwindow_size = 500  # Readings per graph\nnum_graphs = (total_readings + window_size - 1) // window_size  # Number of graphs\n\n# Function to generate spectrograms\ndef window_to_spectrogram(time_values, amplitude_values, T_x, g_std=8, win_length=50, hop_size=10, mfft_length=50, scale_to='magnitude'):\n    x = amplitude_values\n    w = gaussian(win_length, std=g_std, sym=True)\n    SFT = ShortTimeFFT(w, hop=hop_size, fs=1/T_x, mfft=mfft_length, scale_to=scale_to)\n    Sx2 = SFT.stft(x)\n    \n    # Return the spectrogram as a numpy array for further processing\n    return abs(Sx2)  # Only return the magnitude of the spectrogram\n\nspectrograms2 = []\n\n# Process each segment and create spectrograms\nfor i in range(num_graphs):\n    start_idx = i * window_size\n    end_idx = min((i + 1) * window_size, total_readings)\n\n    time_segment2 = time2[start_idx:end_idx].to_numpy()\n    pressure_segment2 = pressure2[start_idx:end_idx].to_numpy()\n\n    # Generate spectrogram and store it\n    print(f\"Processing window SET II {i+1}/{num_graphs}\")\n    Sx2 = window_to_spectrogram(time_segment2, pressure_segment2, T_x=1/50)\n    spectrograms2.append(Sx2)\n\n# Function to divide a spectrogram into 10 equal segments\ndef divide_into_segments(spectrogram, num_segments=10):\n    # Determine the number of rows in the spectrogram\n    num_rows2 = spectrogram.shape[0]\n    \n    # Calculate the number of rows per segment (can be rounded)\n    rows_per_segment2 = num_rows2 // num_segments\n    segments2 = []\n    \n    for i in range(num_segments):\n        start_row2 = i * rows_per_segment2\n        # For the last segment, we take the remaining rows to handle any remainder\n        end_row2 = (i + 1) * rows_per_segment2 if i < num_segments - 1 else num_rows2\n        segment = spectrogram[start_row2:end_row2, :]\n        \n        # Pad the segment to ensure consistent shape (optional, depends on your needs)\n        if segment.shape[0] < rows_per_segment2:\n            pad_amount2 = rows_per_segment2 - segment.shape[0]\n            segment = np.pad(segment, ((0, pad_amount2), (0, 0)), mode='constant')\n        \n        segments2.append(segment)\n    \n    # Ensure all segments are the same shape (same number of rows and columns)\n    max_rows = max([seg.shape[0] for seg in segments2])\n    max_cols = max([seg.shape[1] for seg in segments2])\n    \n    # Pad all segments to the same size\n    padded_segments2 = [np.pad(seg, ((0, max_rows - seg.shape[0]), (0, max_cols - seg.shape[1])), mode='constant') for seg in segments2]\n    \n    return padded_segments2\n\n# Convert spectrograms into 10 x n x n tensors\nspectrogram_tensors_file1 = []\n\n# Process each spectrogram and divide into 10 segments\nfor i, Sx2 in enumerate(spectrograms2):\n    print(f\"Dividing spectrogram SET II {i+1}/{len(spectrograms2)} into 10 segments.\")\n    segments2 = divide_into_segments(Sx2, num_segments=10)\n    \n    # Convert the list of segments into a numpy array (tensor)\n    tensor = np.array(segments2)\n    \n    # Append the tensor to the list\n    spectrogram_tensors_file1.append(tensor)\n\n# Now spectrogram_tensors_file1 contains 200 tensors, each of shape (10, n, n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\nspectrogram_tensors_file0 = [torch.tensor(tensor) if not torch.is_tensor(tensor) else tensor for tensor in spectrogram_tensors_file0]\nspectrogram_tensors_file1 = [torch.tensor(tensor) if not torch.is_tensor(tensor) else tensor for tensor in spectrogram_tensors_file1]\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nlabels_file0 = [0] * len(spectrogram_tensors_file0)\nlabels_file1 = [1] * len(spectrogram_tensors_file1)\n\n\nspectrograms01 = spectrogram_tensors_file0 + spectrogram_tensors_file1\nlabels01 = labels_file0 + labels_file1\n\n\nlabels_tensor01 = torch.tensor(labels01, dtype=torch.long)  # Shape: (400,)\n\n\nclass SpectrogramDataset(Dataset):\n    def __init__(self, spectrograms01, labels01):\n        self.spectrograms01 = spectrograms01\n        self.labels01 = labels01\n\n    def __len__(self):\n        return len(self.spectrograms01)\n\n    def __getitem__(self, idx):\n       \n        spectrogram = self.spectrograms01[idx]\n        label = self.labels01[idx]\n        return spectrogram, label\n\n\ndataset = SpectrogramDataset(spectrograms01, labels_tensor01)\n\n\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n\nfor spectrogram_batch, label_batch in dataloader:\n    print(\"Spectrogram Batch Shape:\", spectrogram_batch.shape)\n    print(\"Label Batch Shape:\", label_batch.shape)\n    break\n\n\n\ndef check_tensor_status():\n    if torch.is_tensor(spectrogram_tensors_file0[0]):  \n        print(\"spectrogram_tensors_file0 is a tensor.\")\n    else:\n        print(\"spectrogram_tensors_file0 is NOT a tensor.\")\n\n    if torch.is_tensor(spectrogram_tensors_file1[0]):  \n        print(\"spectrogram_tensors_file1 is a tensor.\")\n    else:\n        print(\"spectrogram_tensors_file1 is NOT a tensor.\")\n\n\ncheck_tensor_status()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n\n\n\ndataset = SpectrogramDataset(spectrograms01, labels_tensor01)\n\n\ntrain_size = int(0.8 * len(dataset))  \ntest_size = int(0.1 * len(dataset))  \nval_size = len(dataset) - train_size - test_size  \n\n\ntrain_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\ntrain_batch = next(iter(train_loader))\n\n\nspectrogram, label = train_batch\n\n\nprint(f\"Spectrogram batch shape: {spectrogram.shape}\")\nprint(f\"Label batch shape: {label.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n\nclass SpectrogramCNN(nn.Module):\n    def __init__(self, in_channels=10):  \n        super(SpectrogramCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels=32, kernel_size=3, stride=1, padding=1)  \n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1) \n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)  \n        self.fc1 = nn.Linear(128 * 8 * 55, 512)\n        self.fc2 = nn.Linear(512, 2)  \n        \n        \n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        \n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        \n        \n        x = x.view(x.size(0), -1)  \n        \n       \n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x  \n\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n\n    def should_stop(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            return False\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            return False\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n            return False\n\n\ndef save_model(model, path):\n    torch.save(model.state_dict(), path)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n\n\nmodel = SpectrogramCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\nearly_stopping = EarlyStopping(patience=5)\n\n\nbest_val_loss = float('inf')\nsave_path = \"best_spectrogram_cnn.pth\"\n\ntrain_losses = []\nval_losses = []\n\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    correct_preds = 0\n    total_preds = 0\n    \n    \n    for spectrogram_batch, label_batch in train_loader:\n        spectrogram_batch, label_batch = spectrogram_batch.to(device), label_batch.to(device)\n        spectrogram_batch = spectrogram_batch.float()  # Ensure correct data type\n\n        optimizer.zero_grad()\n\n        outputs = model(spectrogram_batch)\n        loss = criterion(outputs, label_batch)\n        train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs, 1)\n        correct_preds += (predicted == label_batch).sum().item()\n        total_preds += label_batch.size(0)\n    \n    train_accuracy = correct_preds / total_preds\n    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss/len(train_loader):.4f} - Training Accuracy: {train_accuracy:.4f}\")\n\n    \n    model.eval()\n    val_loss = 0.0\n    correct_preds = 0\n    total_preds = 0\n    with torch.no_grad():\n        for spectrogram_batch, label_batch in val_loader:\n            spectrogram_batch, label_batch = spectrogram_batch.to(device), label_batch.to(device)\n            spectrogram_batch = spectrogram_batch.float()\n\n            outputs = model(spectrogram_batch)\n            loss = criterion(outputs, label_batch)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct_preds += (predicted == label_batch).sum().item()\n            total_preds += label_batch.size(0)\n\n    val_accuracy = correct_preds / total_preds\n    print(f\"Validation Loss: {val_loss/len(val_loader):.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n\n    \n   \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss \n        save_model(model, save_path)  \n        print(f\"Saved the model with validation loss: {val_loss/len(val_loader):.4f}\")\n\n    train_losses.append(train_loss / len(train_loader))\n    val_losses.append(val_loss / len(val_loader))\n\n    \n    \n    if early_stopping.should_stop(val_loss):\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\n\nimport numpy as np\nnp.savetxt(\"train_losses.csv\", train_losses, delimiter=\",\")\nnp.savetxt(\"val_losses.csv\", val_losses, delimiter=\",\")\n\n\n\nmodel.eval()\ncorrect_preds = 0\ntotal_preds = 0\nwith torch.no_grad():\n    for spectrogram_batch, label_batch in test_loader:\n        spectrogram_batch, label_batch = spectrogram_batch.to(device), label_batch.to(device)\n        spectrogram_batch = spectrogram_batch.float()\n\n        outputs = model(spectrogram_batch)\n        _, predicted = torch.max(outputs, 1)\n\n        correct_preds += (predicted == label_batch).sum().item()\n        total_preds += label_batch.size(0)\n\ntest_accuracy = 100 * correct_preds / total_preds\nprint(f\"Training Accuracy: {test_accuracy}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n\ntrain_losses = np.loadtxt(\"train_losses.csv\", delimiter=\",\")\nval_losses = np.loadtxt(\"val_losses.csv\", delimiter=\",\")\n\n\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curves')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n\nclass SpectrogramCNN(nn.Module):\n    def __init__(self, in_channels=10):  \n        super(SpectrogramCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1, padding=1)  \n        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)  \n        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)  \n        self.fc1 = nn.Linear(128 * 8 * 55, 512)\n        self.fc2 = nn.Linear(512, 2)  \n        \n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)  \n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\ndef load_model(path, device):\n    model = SpectrogramCNN().to(device)  \n    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))  \n    model.eval()\n    return model\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nmodel = load_model(\"best_spectrogram_cnn.pth\", device)\n\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n\ncorrect_count = 0\nincorrect_count = 0\n\n\nmodel.eval()  \nwith torch.no_grad():\n    for spectrogram_batch, label_batch in test_loader:\n        spectrogram_batch, label_batch = spectrogram_batch.to(device), label_batch.to(device)\n        spectrogram_batch = spectrogram_batch.float()\n\n        outputs = model(spectrogram_batch)\n        _, predicted = torch.max(outputs, 1)  \n        \n        \n        for true, pred in zip(label_batch, predicted):\n            if true == pred:\n                correct_count += 1\n            else:\n                incorrect_count += 1\n            print(f\"True Label: {true.item()}, Predicted Label: {pred.item()}\")\n\n\ntotal_count = correct_count + incorrect_count\naccuracy = 100 * correct_count / total_count\nprint(f\"Correct Count: {correct_count}\")\nprint(f\"Incorrect Count: {incorrect_count}\")\nprint(f\"Test Accuracy: {accuracy:}%\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.video import r3d_18, R3D_18_Weights\nfrom torchvision import transforms\nimport numpy as np\n\n# Define the ResNet model for video input\nclass VideoResNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(VideoResNet, self).__init__()\n        \n        # Initialize the pretrained 3D ResNet model\n        self.resnet = r3d_18(weights=R3D_18_Weights.DEFAULT)\n        \n        # Freeze all layers except the fully connected layer\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n        \n        # Modify the final fully connected layer\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n\n\n\n    \n    def forward(self, x):\n        return self.resnet(x)\n\n# Early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n\n    def should_stop(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            return False\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            return False\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n            return False\n\n# Save model function\ndef save_model(model, path):\n    torch.save(model.state_dict(), path)\n\n# Model and training setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data loaders for train, validation, and test datasets\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Initialize model, loss function, and optimizer\nmodel = VideoResNet(num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Early stopping and best model tracking\nearly_stopping = EarlyStopping(patience=5)\nbest_val_loss = float('inf')\nsave_path = \"best_video_resnet.pth\"\n\ntrain_losses = []\nval_losses = []\n\n# Training loop\ndef convert_tensor_to_3d(matrix):\n    #matrix =  matrix.type(torch.double)\n    matrix = matrix.type(torch.float)\n    matrix_converted = matrix.unsqueeze(2)  \n    matrix_converted = matrix_converted.repeat(1, 1, 3, 1, 1)  \n\n    matrix_flattened = matrix_converted.view(-1,3,matrix_converted.shape[3], matrix_converted.shape[4])\n    resize_transform = transforms.Resize([112,112])\n    matrix_flattened = resize_transform(matrix_flattened)\n\n    final_matrix = matrix_flattened.view(-1,matrix_converted.shape[1], 3, matrix_flattened.shape[2], matrix_flattened.shape[3])\n    final_matrix = torch.transpose(final_matrix,1,2)\n    return final_matrix\n    '''\n    matrix_converted = F.interpolate(matrix_converted.view(-1, 3, matrix_converted.shape[3], matrix_converted.shape[4]),\n                                    size=(112, 112),\n                                 mode='bilinear',\n                                    align_corners=False)\n    #matrix_converted = matrix_converted.view(-1, 10, 3, 112, 112)\n    \n    return matrix_converted.double()\n    '''\n\n\n\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    correct_preds = 0\n    total_preds = 0\n    \n    for spectrogram_batch_resized, label_batch in train_loader:\n        spectrogram_batch_resized, label_batch = spectrogram_batch_resized.to(device), label_batch.to(device)\n        spectrogram_batch_resized = convert_tensor_to_3d(spectrogram_batch_resized)\n        #print(spectrogram_batch_resized.shape)\n        #print(spectrogram_batch_resized.dtype)\n        optimizer.zero_grad()\n\n        outputs = model(spectrogram_batch_resized.float())\n        loss = criterion(outputs, label_batch)\n        train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs, 1)\n        correct_preds += (predicted == label_batch).sum().item()\n        total_preds += label_batch.size(0)\n    \n    train_accuracy = 100 * correct_preds / total_preds\n    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss/len(train_loader):.4f} - Training Accuracy: {train_accuracy:.4f}\")\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct_preds = 0\n    total_preds = 0\n    with torch.no_grad():\n        for spectrogram_batch_resized, label_batch in val_loader:\n            spectrogram_batch_resized, label_batch = spectrogram_batch_resized.to(device), label_batch.to(device)\n            spectrogram_batch_resized = convert_tensor_to_3d(spectrogram_batch_resized)\n            #print(spectrogram_batch_resized.shape)\n            #print(spectrogram_batch_resized.dtype)\n            outputs = model(spectrogram_batch_resized)\n            loss = criterion(outputs, label_batch)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct_preds += (predicted == label_batch).sum().item()\n            total_preds += label_batch.size(0)\n\n    val_accuracy = correct_preds / total_preds\n    print(f\"Validation Loss: {val_loss/len(val_loader):.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n\n    # Save the best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_model(model, save_path)\n        print(f\"Saved the model with validation loss: {val_loss/len(val_loader):.4f}\")\n\n    train_losses.append(train_loss / len(train_loader))\n    val_losses.append(val_loss / len(val_loader))\n\n    # Check for early stopping\n    if early_stopping.should_stop(val_loss):\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\n# Save losses\nnp.savetxt(\"train_losses_video_resnet.csv\", train_losses, delimiter=\",\")\nnp.savetxt(\"val_losses_video_resnet.csv\", val_losses, delimiter=\",\")\n\n# Test loop\nmodel.eval()\ncorrect_preds = 0\ntotal_preds = 0\nwith torch.no_grad():\n    for spectrogram_batch_resized, label_batch in test_loader:\n        spectrogram_batch_resized, label_batch = spectrogram_batch_resized.to(device), label_batch.to(device)\n        spectrogram_batch_resized = convert_tensor_to_3d(spectrogram_batch_resized)\n        #print(spectrogram_batch_resized.shape)\n        #print(spectrogram_batch_resized.dtype)\n\n        outputs = model(spectrogram_batch_resized)\n        _, predicted = torch.max(outputs, 1)\n\n        correct_preds += (predicted == label_batch).sum().item()\n        total_preds += label_batch.size(0)\n\ntest_accuracy = 100 * correct_preds / total_preds\nprint(f\"Training Accuracy using Video ResNet: {test_accuracy}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n\ncorrect_count = 0\nincorrect_count = 0\n\n\nmodel.eval()  \nwith torch.no_grad():\n    for spectrogram_batch_resized, label_batch in test_loader:\n        spectrogram_batch_resized, label_batch = spectrogram_batch_resized.to(device), label_batch.to(device)\n        spectrogram_batch_resized = convert_tensor_to_3d(spectrogram_batch_resized)\n        \n        outputs = model(spectrogram_batch_resized)\n        _, predicted = torch.max(outputs, 1)  \n        \n        \n        for true, pred in zip(label_batch, predicted):\n            if true == pred:\n                correct_count += 1\n            else:\n                incorrect_count += 1\n            print(f\"True Label: {true.item()}, Predicted Label: {pred.item()}\")\n\n\ntotal_count = correct_count + incorrect_count\naccuracy = 100 * correct_count / total_count\nprint(f\"Correct Count: {correct_count}\")\nprint(f\"Incorrect Count: {incorrect_count}\")\nprint(f\"Test Accuracy using Video ResNet: {accuracy:}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}